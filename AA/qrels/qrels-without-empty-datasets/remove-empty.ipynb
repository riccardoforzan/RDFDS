{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove empty\n",
    "Generate a new collection that does not contain any empty dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"../datasets\"                          # source folder\n",
    "source_qrels = \"qrels.txt\"                      # source qrels file\n",
    "modified_qrels = \"qrels_without_empty.txt\"\n",
    "empty_datasets_ids = \"empty.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = sorted(os.listdir(source), key=lambda i: int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class DatasetType(Enum):\n",
    "    EMPTY = 0\n",
    "    PARTIAL = 1\n",
    "    COMPLETE = 2\n",
    "\n",
    "def analyze_dataset(dataset_path) -> DatasetType:\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        metadata = json.load(f, strict=False)\n",
    "\n",
    "        # check if the dataset has been downloaded completely\n",
    "        error_while_downloading = len(metadata[\"failedURLs\"]) > 0\n",
    "\n",
    "        # check if the file dataset contains at least one file that has been parsed\n",
    "        contains_a_valid_file = len(metadata[\"extracted\"]) > 0\n",
    "\n",
    "        # check if the dataset has some files that have not been parsed or has thrown errors while parsing\n",
    "        error_while_parsing = len(metadata[\"unusedFiles\"]) > 0\n",
    "\n",
    "        \"\"\" \n",
    "        A dataset is complete only if all these conditions are satisfied:\n",
    "        1) contains at least one valid file (>0)\n",
    "        2) has been completely downloaded\n",
    "        3) no file has generated error while parsing\n",
    "        \"\"\"\n",
    "\n",
    "        if contains_a_valid_file and not error_while_downloading and not error_while_parsing:\n",
    "            return DatasetType.COMPLETE\n",
    "\n",
    "        \"\"\"\n",
    "        A dataset is partial if:\n",
    "        1) contains at least one valid file (>0)\n",
    "        2) some files may not have been downloaded\n",
    "        3) some files may have generated errors or not being the correct type to be used\n",
    "        \"\"\"\n",
    "\n",
    "        if contains_a_valid_file:\n",
    "            return DatasetType.PARTIAL\n",
    "\n",
    "        \"\"\"\n",
    "        If a dataset doesn't contain any file\n",
    "        \"\"\"\n",
    "        return DatasetType.EMPTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty = list()\n",
    "empty_datasets = list()\n",
    "\n",
    "for dataset in datasets:\n",
    "    metadata_file_path = f\"{source}/{dataset}/metadata.json\"\n",
    "    res = analyze_dataset(metadata_file_path)\n",
    "\n",
    "    if res == DatasetType.COMPLETE or res == DatasetType.PARTIAL:\n",
    "        non_empty.append(dataset)\n",
    "    \n",
    "    if res == DatasetType.EMPTY:\n",
    "        empty_datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5201"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(empty_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(empty_datasets_ids, 'w') as empty_file:\n",
    "    for id in empty_datasets:\n",
    "        empty_file.write(\"\".join(id) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the modified QRELS file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the input file for reading\n",
    "with open(source_qrels, 'r') as input_file:\n",
    "    # Read the lines from the input file\n",
    "    lines = input_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lines = [line for line in lines if line.split()[2] not in empty_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the output file for writing\n",
    "with open(modified_qrels, 'w') as output_file:\n",
    "    # Write the filtered lines to the output file\n",
    "    output_file.writelines(filtered_lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
