{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats\n",
    "Statistics about downloaded and processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the relative path or the absolute path pointing to the directory in which datasets have been downloaded\n",
    "folder = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_datasets = list()  # completely downloaded\n",
    "partial_datasets = list()  # not completely downloaded / parsed (at least 1 valid file)\n",
    "empty_datasets = list()  # only metadata for these datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan the directory containing the downloaded datasets\n",
    "datasets = sorted(os.listdir(folder), key=lambda i: int(i))\n",
    "total_datasets = len(datasets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SIZE_LIMIT = 200 * 1024 * 1024  # 100 MB\n",
    "\n",
    "\n",
    "def is_file_larger_than_size_limit(filepath: str) -> bool:\n",
    "    size = os.path.getsize(str(filepath))\n",
    "    return int(size) >= int(SIZE_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDF_SUFFIXES = [\"rdf\", \"ttl\", \"owl\", \"n3\", \"nt\", \"jsonld\", \"nq\", \"trig\", \"trix\"]\n",
    "\n",
    "\n",
    "def check_if_file_name_is_rdf(name: str) -> bool:\n",
    "    return name.split(\".\")[-1] in RDF_SUFFIXES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import magic\n",
    "\n",
    "\n",
    "def is_html(filepath: str) -> bool:\n",
    "    mt = magic.from_file(filepath).lower()\n",
    "    if \"html\" in mt:\n",
    "        return True\n",
    "\n",
    "    with open(filepath, \"r\") as f:\n",
    "        try:\n",
    "            return \"<!doctype html\" in f.read().lower()\n",
    "        except Exception:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def is_json(filepath: str) -> bool:\n",
    "    with open(filepath, \"r\") as f:\n",
    "        try:\n",
    "            json.load(f)\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_file(file_path: str):\n",
    "    if os.path.isfile(file_path):\n",
    "        print(f\"Deleting {file_path}\")\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the processing status for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class DatasetType(Enum):\n",
    "    EMPTY = 0\n",
    "    PARTIAL = 1\n",
    "    COMPLETE = 2\n",
    "\n",
    "\n",
    "def analyze_dataset(dataset_path) -> DatasetType:\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        metadata = json.load(f, strict=False)\n",
    "\n",
    "        # check if the dataset has been downloaded completely\n",
    "        error_while_downloading = len(metadata[\"failedURLs\"]) > 0\n",
    "\n",
    "        # check if the file dataset contains at least one file that has been parsed\n",
    "        contains_a_valid_file = len(metadata[\"extracted\"]) > 0\n",
    "\n",
    "        # check if the dataset has some files that have not been parsed or has thrown errors while parsing\n",
    "        error_while_parsing = len(metadata[\"unusedFiles\"]) > 0\n",
    "\n",
    "        \"\"\" \n",
    "        A dataset is complete only if all these conditions are satisfied:\n",
    "        1) contains at least one valid file (>0)\n",
    "        2) has been completely downloaded\n",
    "        3) no file has generated error while parsing\n",
    "        \"\"\"\n",
    "\n",
    "        if contains_a_valid_file and not error_while_downloading and not error_while_parsing:\n",
    "            return DatasetType.COMPLETE\n",
    "\n",
    "        \"\"\"\n",
    "        A dataset is partial if:\n",
    "        1) contains at least one valid file (>0)\n",
    "        2) some files may not have been downloaded\n",
    "        3) some files may have generated errors or not being the correct type to be used\n",
    "        \"\"\"\n",
    "\n",
    "        if contains_a_valid_file:\n",
    "            return DatasetType.PARTIAL\n",
    "\n",
    "        \"\"\"\n",
    "        If a dataset doesn't contain any file\n",
    "        \"\"\"\n",
    "        return DatasetType.EMPTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    metadata_file_path = f\"{folder}/{dataset}/metadata.json\"\n",
    "\n",
    "    res = analyze_dataset(metadata_file_path)\n",
    "\n",
    "    if res == DatasetType.COMPLETE:\n",
    "        complete_datasets.append(dataset)\n",
    "\n",
    "    if res == DatasetType.PARTIAL:\n",
    "        partial_datasets.append(dataset)\n",
    "\n",
    "    if res == DatasetType.EMPTY:\n",
    "        empty_datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of datasets: {total_datasets}\")\n",
    "print(f\"Complete datasets: {len(complete_datasets)}\")\n",
    "print(f\"Partial datasets: {len(partial_datasets)}\")\n",
    "print(f\"Empty datasets: {len(empty_datasets)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List datasets with unused file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_with_unused_files = list()\n",
    "\n",
    "for dataset in datasets:\n",
    "    metadata_file_path = f\"{folder}/{dataset}/metadata.json\"\n",
    "\n",
    "    with open(metadata_file_path, \"r\") as f:\n",
    "        metadata = json.load(f, strict=False)\n",
    "\n",
    "        keys = metadata.keys()\n",
    "\n",
    "        if \"unused_files\" in keys and len(metadata[\"unused_files\"]) > 0:\n",
    "            unparsable_rdf = list()\n",
    "            unparsable_other = list()\n",
    "\n",
    "            for file in metadata[\"unused_files\"]:\n",
    "                file_with_path = f\"{folder}/{dataset}/{file}\"\n",
    "\n",
    "                if check_if_file_name_is_rdf(file):\n",
    "                    unparsable_rdf.append(file)\n",
    "                else:\n",
    "                    unparsable_other.append(file)\n",
    "            \n",
    "            datasets_with_unused_files.append([dataset, unparsable_rdf, unparsable_other])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "markdown_table = \"\"\"\n",
    "| Dataset ID | RDF not parsable | Other not parsable |\n",
    "| --- | --- | --- |\n",
    "\"\"\"\n",
    "\n",
    "for d in datasets_with_unused_files:\n",
    "    markdown_table += (\"| {} | {} | {} |\\n\".format(d[0], str(d[1]), str(d[2])))\n",
    "\n",
    "display(Markdown(markdown_table))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing unused files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_used_files = 0\n",
    "big_files = list()\n",
    "unusable_files = list()\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset_folder_path = f\"{folder}/{dataset}\"\n",
    "    metadata_file_path = f\"{dataset_folder_path}/metadata.json\"\n",
    "\n",
    "    with open(metadata_file_path, \"r\") as f:\n",
    "        metadata = json.load(f, strict=False)\n",
    "\n",
    "        keys = metadata.keys()\n",
    "\n",
    "        if \"used_files\" in keys and len(metadata[\"used_files\"]) > 0:\n",
    "            total_used_files += len(metadata[\"used_files\"])\n",
    "\n",
    "        if \"unused_files\" in keys and len(metadata[\"unused_files\"]) > 0:\n",
    "            for uf in metadata[\"unused_files\"]:\n",
    "                file_path = f\"{dataset_folder_path}/{uf}\"\n",
    "                if is_file_larger_than_size_limit(file_path):\n",
    "                    big_files.append(file_path)\n",
    "                else:\n",
    "                    unusable_files.append(file_path)\n",
    "\n",
    "print(f\"Total used files: {total_used_files}\")\n",
    "print(f\"Big files: {len(big_files)}\")\n",
    "print(f\"Unusable files: {len(unusable_files)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unused files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unusable_files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "diff datasets/2/ppg-sf-dump.rdf datasets/6/ppg-sf-dump.rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "diff datasets/13263/Govwild_rdf.n3 datasets/14364/Govwild_rdf.n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "diff datasets/13347/geospecies.rdf datasets/14324/geospecies.rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cmp datasets/13368/all-geonames.rdf datasets/14344/all-geonames.rdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicates\n",
    "- `datasets/2/ppg-sf-dump.rdf` is the same file as `datasets/6/ppg-sf-dump.rdf`\n",
    "- `datasets/13263/Govwild_rdf.n3` is the same file as `datasets/14364/Govwild_rdf.n3`\n",
    "- `datasets/13347/geospecies.rdf` is the same file as `datasets/14324/geospecies.rdf`\n",
    "- `datasets/13368/all-geonames.rdf` is the same file as `datasets/14344/all-geonames.rdf`\n",
    "\n",
    "#### Invalid files\n",
    "While importing in GraphDB the files below generated errors\n",
    "- `datasets/13263/Govwild_rdf.n3` contains syntax error, GraphDB raises `org.eclipse.rdf4j.sail.SailException: Invalid IRI value`\n",
    "- `datasets/13347/geospecies.rdf` contains syntax error, GraphDB raises `org.eclipse.rdf4j.sail.SailException: Invalid IRI value`\n",
    "- `datasets/13368/all-geonames.rdf` is not processable, GraphDB raises `RDF parse error: content is not allowed in prolog`\n",
    "- `datasets/13565/download-20120123.rdf` contains syntax error, GraphDB raises `org.eclipse.rdf4j.sail.SailException: Invalid IRI value`\n",
    "- `datasets/15243/fr.rdf` contains syntax error, GraphDB raises `RDF parse error`\n",
    "- `datasets/21532/jrcnames_uri.nt` contains syntax error, GraphDB raises `org.eclipse.rdf4j.sail.SailException: Invalid IRI value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_valid_big_files = [\n",
    "    \"datasets/2/ppg-sf-dump.rdf\",\n",
    "    \"datasets/11580/rows.rdf\",\n",
    "    \"datasets/14079/eat.nt\",\n",
    "    \"datasets/15243/en.rdf\",\n",
    "    \"datasets/21023/2016-allievi-partecipanti.nt\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in big_files:\n",
    "    if file not in distinct_valid_big_files:\n",
    "        delete_file(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
